{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jugernaut/Induccion_MeIA/blob/angel/4_IntroduccionAA/03_MetodoDeNewton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUjCEqICmjLt"
   },
   "source": [
    "# Método de Newton\n",
    "\n",
    "Método de Newton</a> by <span property=\"cc:attributionName\">Miguel Angel Pérez León</span> is licensed under <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1\" target=\"_blank\" rel=\"license noopener noreferrer\" style=\"display:inline-block;\">CC BY-NC-SA 4.0<img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\"></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u2PPQpOmjMF"
   },
   "source": [
    "## Método de Newton\n",
    "\n",
    "El método de Newton hace uso de la definición de la derivada de $f(x)$ para encontrar la aproximación a la raíz de la función.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0E4479gQ64X"
   },
   "source": [
    "### Descripción del método\n",
    "\n",
    "* El método de Newton comienza con una aproximación inicial $x_{0}$ a la raíz, a continuación se traza una recta tangente a la curva en el punto $\\left(x_{0},\\,f\\left(x_{0}\\right)\\right)$. La intersección de la recta tangente con el eje x, se denota como $x_{1}$ y se considera una mejor aproximación de la raíz.\n",
    "\n",
    "\n",
    "* Después se traza otra recta tangente a la curva en el punto $\\left(x_{1},\\,f\\left(x_{1}\\right)\\right)$. La nueva intersección de la recta tangente con el eje $x$, se denota como $x_{2}$ y se considera una mejor aproximación de la raíz.\n",
    "\n",
    "\n",
    "* El proceso se repite hasta que se cumplan los criterios de convergencia de las ecuaciones.\n",
    "\n",
    "\n",
    "* El método de Newton, se puede aplicar para hallar raíces complejas, siempre y cuando el valor inicial $x_{0}$ sea un número complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol1Uc6ejQhes"
   },
   "source": [
    "### Algoritmo\n",
    "\n",
    "La intersección de la recta tangente con el eje $x$ se puede calcular de la siguiente manera:\n",
    "\n",
    "\n",
    "$$\\tan \\theta\t=\t\\frac{cateto \\hspace{1mm}opuesto}{cateto\\hspace{1mm}adyacente}$$\n",
    "\n",
    "$$\\tan \\theta=\\frac{f\\left(x_{0}\\right)}{x_{0}-x_{1}}=f^{'}(x_{0})$$\n",
    "\n",
    "Se iguala a la primera derivada de $x_{0}$, ya que ésta es equivalente a la pendiente :\n",
    "\n",
    "$$f^{'}\\left(x_{0}\\right) = \\frac{f\\left(x_{0}\\right)-0}{x_{0}-x_{1}}$$\n",
    "\n",
    "$$ \\Rightarrow \\frac{f\\left(x_{0}\\right)}{x_{0}-x_{1}} = f^{'}(x_{0})$$\n",
    "\n",
    "$$ \\left(x_{0}-x_{1}\\right)f^{'}\\left(x_{0}\\right) = f(x_{0})$$\n",
    "\n",
    "$$ f\\left(x_{0}\\right) = x_{0}f^{'}\\left(x_{0}\\right)-x_{1}f^{'}\\left(x_{0}\\right)$$\n",
    "\n",
    "$$ x_{1}f^{'}\\left(x_{0}\\right) = x_{0}f^{'}\\left(x_{0}\\right)-f\\left(x_{0}\\right)$$\n",
    "\n",
    "$$ x_{1} = x_{0}-\\frac{f\\left(x_{0}\\right)}{f^{'}\\left(x_{0}\\right)}$$\n",
    "\n",
    "**Método iterativo de Newton**\n",
    "\n",
    "Para obtener la n-esima intersección con el eje $x$ de la recta tangente a la curva, la ecuación sería:\n",
    "\n",
    "$$ x_{n}=x_{n-1}-\\frac{f\\left(x_{n-1}\\right)}{f^{'}\\left(x_{n-1}\\right)}$$\n",
    "\n",
    "**Pseudocódigo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tW9LMWrWmjMG"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Figuras/raicesNumericas/algNew.PNG?raw=1\" width=\"700\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEzzgezumjMH"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Figuras/raicesNumericas/esqNew.PNG?raw=1\" width=\"700\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9OnGKC6tmjMH",
    "outputId": "03ab4c72-4543-494e-e42f-1befcfc37765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La raíz con el método de Newton es: 1.0000859282498527\n"
     ]
    }
   ],
   "source": [
    "'''Esta función implementa el método de la falsa posición\n",
    "para encontrar la raiz de una funcion.\n",
    "f:   función de la cual se busca la raiz\n",
    "df:  derivada de f\n",
    "Tol: tolerancia del error numerico\n",
    "N:   numero maximo de iteraciones\n",
    "x0:  aproximación inicial\n",
    "'''\n",
    "def newton(f,df,Tol,N,x0):\n",
    "    #contador de iteraciones\n",
    "    n=1\n",
    "    #mientras no se haya superado el límite de iteraciones\n",
    "    while n<=N:\n",
    "        #se evalua la función y su derivada\n",
    "        fx=f(x0)\n",
    "        dfx=df(x0)\n",
    "        #se calcula la siguiente aproximación\n",
    "        xn = x0-(fx/float(dfx))\n",
    "        #en caso de cumplir criterios se devuelve la raiz\n",
    "        if abs(f(xn)) <= Tol and abs(xn-x0) <= Tol:\n",
    "            return xn\n",
    "        #actualizamos las aproximaciones\n",
    "        x0 = xn\n",
    "        #se incrementa el contador de iteraciones\n",
    "        n=n+1\n",
    "    raise Exception(\"Se alcanzo el maximo numero de iteraciones y no se encontro raiz\")\n",
    "##Faltaba la función\n",
    "def f(x):\n",
    "    return x - 1\n",
    "\n",
    "def df(x):\n",
    "    return 2*x\n",
    "\n",
    "raiz = newton(f,df,0.0001,100,2)\n",
    "print(\"La raíz con el método de Newton es:\", raiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFrFRaI9TJWf"
   },
   "source": [
    "### Ventajas y Desventajas\n",
    "\n",
    "**Ventajas**.\n",
    "\n",
    "* La rapidez de convergencia iterativa del método de Newton es mayor a cualquier método visto previamente.\n",
    "\n",
    "* Sirve para encontrar raíces complejas, para lo cual requiere que $x_0$ sea un número complejo $x_0 = a + bi$.\n",
    "\n",
    "**Desventajas**.\n",
    "\n",
    "* El método requiere una buena estimación inicial. De otro modo, la solución iterativa puede ser divergente o converger a una solución irrelevante.\n",
    "\n",
    "* La derivada $f′(x)$, no siempre es fácil de calcular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQyDCHd1Bxho"
   },
   "source": [
    "### Observaciones\n",
    "\n",
    "Ya que la naturaleza de las funciones puede ser muy variada, hay que tomar en cuenta las siguientes observaciones:\n",
    "\n",
    "\n",
    "*   Si se tiene una aproximación inicial $x_0$ en un punto de inflexión, es decir $f''(x)=0$, las aproximaciones pueden oscilar de manera indefinida.\n",
    "*   Si alguna de las aproximaciones se encuentra cerca de un máximo o un mínimo local. Una pendiente nula provoca una división entre cero en el método (geométricamente esto significa una recta tangente que nunca se intersecta con el eje $x$).\n",
    "*   Si se tiene varias raíces en un intervalo cercano, puede darse el caso de que una aproximación pase de la aproximación de una raíz a otra continuamente, sin lograr la convergencia.\n",
    "\n",
    "<img src=\"https://github.com/jugernaut/Numerico2021/blob/desarrollo/Figuras/raicesNumericas/Newton1.png?raw=1\" width=\"500\">\n",
    "\n",
    "<img src=\"https://github.com/jugernaut/Numerico2021/blob/desarrollo/Figuras/raicesNumericas/Newton2.png?raw=1\" width=\"500\">\n",
    "\n",
    "<img src=\"https://github.com/jugernaut/Numerico2021/blob/desarrollo/Figuras/raicesNumericas/Newton3.png?raw=1\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh716bKucxjA"
   },
   "source": [
    "## Método de Newton (multidimensional)\n",
    "\n",
    "Múltiples problemas de las ciencias exactas implican la resolución de sistemas de $N$ ecuaciones no lineales con $N$ incógnitas\n",
    "\n",
    "Un método es linealizar y resolver, repetidamente. Esta es la misma estrategia que usa el método de Newton para resolver una sola ecuación no lineal.\n",
    "\n",
    "Así que es natural pensar en una forma de extender esta forma de resolver ecuaciones no lineales, de manera que sea posible resolver un sistema de ecuaciones no lineales.\n",
    "\n",
    "En el caso general, un sistema de *N ecuaciones no lineales con N incógnitas* $x_{i}$ se puede presentar en la forma.\n",
    "\n",
    "$$Sistema = \\begin{cases}\n",
    "f_{1}(x_{1},x_{2},\\ldots,x_{n}) & =0\\\\\n",
    "f_{2}(x_{1},x_{2},\\ldots,x_{n}) & =0 \\\\\n",
    "\\vdots \\\\\n",
    "f_{n}(x_{1},x_{2},\\ldots,x_{n}) & =0\n",
    "\\end{cases}$$\n",
    "\n",
    "En caso de que el sistema fuera de ecuaciones lineales, podemos representar este sistema de *N ecuaciones con N incógnitas* mediante su forma matricial.\n",
    "\n",
    "$$A\\vec{x}=\\vec{0}$$\n",
    "\n",
    "Con $A\\in M_{n\\times n}$ y $\\vec{x}, \\vec{b} \\in \\mathbb{R}^{n}$, es decir.\n",
    "\n",
    "$$\\left(\\begin{array}{ccccccc}\n",
    "a_{11} & a_{12} & \\cdots & \\cdots & \\cdots & \\cdots & a_{1n}\\\\\n",
    "a_{21} & a_{22} & \\cdots & \\cdots & \\cdots & \\cdots & a_{2n}\\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots\\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots\\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots\\\\\n",
    "a_{n1} & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & a_{nn}\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "\\vdots\\\\\n",
    "\\vdots\\\\\n",
    "\\vdots\\\\\n",
    "x_{n}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "\\vdots\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "Usando notación vectorial, podemos reescribir el sistema en una forma más elegante:\n",
    "\n",
    "$$ F(\\vec{X})=\\vec{0}$$\n",
    "\n",
    "Definiendo vectores columna como\n",
    "\n",
    "$$F\t=\t\\left[f_{1},f_{2},\\ldots,f_{N}\\right]^{T}$$\n",
    "\n",
    "$$ \\vec{X}\t=\t\\left[x_{1,}x_{2},\\ldots,x_{N}\\right]$$\n",
    "\n",
    "$$ \\vec{0}\t=\t\\left[x_{1,}x_{2},\\ldots,x_{N}\\right]^{T}$$\n",
    "\n",
    "Si recordamos la expresión del método de Newton para ecuaciones no lineales es.\n",
    "\n",
    "$$ x_{k+1}=x_{k}-\\frac{f\\left(x_{k}\\right)}{f^{'}\\left(x_{k}\\right)}$$\n",
    "\n",
    "Donde el subíndice $k$ indica el número de la iteración actual.\n",
    "\n",
    "No es muy difícil extender la expresión anterior a sistemas de ecuaciones no lineales, lo que nos da como resultado la forma iterativa para resolver sistemas de ecuaciones no lineales mediante el método de Newton.\n",
    "\n",
    "$$\\vec{X}^{\\left(k+1\\right)}=\\vec{X}^{\\left(k\\right)}-\\left[F'\\left(\\vec{X}^{(k)}\\right)\\right]^{-1}F\\left(\\vec{X}^{(k)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORb_b64xcxjA"
   },
   "source": [
    "## Desarrollo\n",
    "\n",
    "El desarrollo para llegar al método de Newton para sistemas de ecuaciones lineales se mostrara a continuación y para ello tomaremos las siguientes 3 ecuaciones no lineales\n",
    "\n",
    "$$Sistema =\\begin{cases}\n",
    "f_{1}(x_{1},x_{2},x_{3}) & =0\\\\\n",
    "f_{2}(x_{1},x_{2},x_{3}) & =0 \\quad \\tag{1}\\\\\n",
    "f_{3}(x_{1},x_{2},x_{3}) & =0\n",
    "\\end{cases}$$\n",
    "\n",
    "Si se aplica el desarrollo del polinomio de Taylor centrado en $\\vec{0}$ para 3 variables $i=1,2,3$, se tiene una solución aproximada para cada $f_{i}$.\n",
    "\n",
    "$$P_{i}(x_{1},x_{2},x_{3}) \\thickapprox f_{i}(0,0,0)+\\frac{\\delta f_{i}}{\\delta x_{1}}x_{1}+\\frac{\\delta f_{i}}{\\delta x_{2}}x_{2}+\\frac{\\delta f_{i}}{\\delta x_{3}}x_{3}$$\n",
    "\n",
    "Supongamos que el vector $\\vec{X}^{(0)}=(x_{1}^{(0)},x_{2}^{(0)},x_{3}^{(0)})$ es una solución aproximada de (1). Sea $\\vec{H}= \\left(h_{1},h_{2},h_{3}\\right)$ una corrección de la solución inicial, de modo que.\n",
    "\n",
    "$$\\vec{X}^{(1)}=\\vec{X}^{(0)}+\\vec{H}=(x_{1}^{(0)}+h_{1},x_{2}^{(0)}+h_{2},x_{3}^{(0)}+h_{3})$$\n",
    "\n",
    "La aproximación $\\vec{X}^{(1)}$ es una mejor aproximación y además.\n",
    "\n",
    "$$\\vec{H}=\\vec{X}^{(1)}-\\vec{X}^{(0)}=(x_{1}^{(1)}-x_{1}^{(0)},x_{2}^{(1)}-x_{2}^{(0)},x_{3}^{(1)}-x_{3}^{(0)})$$\n",
    "\n",
    "Descartando los términos de orden superior en el desarrollo de Taylor, una mejor aproximación esta dada por.\n",
    "\n",
    "$$f_{i}(x_{1}^{(1)},x_{2}^{(1)},x_{3}^{(1)})=f_{i}(x_{1}^{(0)},x_{2}^{(0)},x_{3}^{(0)})+\\frac{\\delta f_{i}}{\\delta x_{1}}(x_{1}^{(1)}-x_{1}^{(0)})+\\frac{\\delta f_{i}}{\\delta x_{2}}(x_{2}^{(1)}-x_{2}^{(0)})+\\frac{\\delta f_{i}}{\\delta x_{3}}(x_{3}^{(1)}-x_{3}^{(0)})$$\n",
    "\n",
    "Reescribiendo la ecuación anterior de manera más compacta.\n",
    "\n",
    "$$f_{i}(x_{1}+h_{1},x_{2}+h_{2},x_{3}+h_{1})=f_{i}(x_{1},x_{2},x_{3})+\\frac{\\delta f_{i}}{\\delta x_{1}}h_{1}+\\frac{\\delta f_{i}}{\\delta x_{2}}h_{2}+\\frac{\\delta f_{i}}{\\delta x_{3}}h_{3}\\tag{2} $$\n",
    "\n",
    "Ya que en $(1)$ el sistema esta igualado a $\\vec{0}$, entonces.\n",
    "\n",
    "$$f_{i}(x_{1}+h_{1},x_{2}+h_{2},x_{3}+h_{1})\\thickapprox 0$$\n",
    "\n",
    "En notación vectorial, se tiene.\n",
    "\n",
    "$$\\vec{0}\\thickapprox F\\left(\\vec{X}^{(0)}+\\vec{H}\\right)\\thickapprox F\\left(\\vec{X}^{(0)}\\right)+F'\\left(\\vec{X}^{(0)}\\right)\\vec{H} \\tag{3}$$\n",
    "\n",
    "Donde la **matriz Jacobiana ó Jacobiano**, esta definida por\n",
    "\n",
    "$$F'\\left(\\vec{X}^{(0)}\\right)=\\left[\\begin{array}{ccc}\n",
    "\\frac{\\delta f_{1}}{\\delta x_{1}} & \\frac{\\delta f_{1}}{\\delta x_{2}} & \\frac{\\delta f_{1}}{\\delta x_{3}}\\\\\n",
    "\\frac{\\delta f_{2}}{\\delta x_{1}} & \\frac{\\delta f_{2}}{\\delta x_{2}} & \\frac{\\delta f_{2}}{\\delta x_{3}}\\\\\n",
    "\\frac{\\delta f_{3}}{\\delta x_{1}} & \\frac{\\delta f_{3}}{\\delta x_{2}} & \\frac{\\delta f_{3}}{\\delta x_{3}}\n",
    "\\end{array}\\right] $$\n",
    "\n",
    "En la primera iteración todas las derivadas parciales se evalúan en $\\vec{X}^{(0)}$, es decir.\n",
    "\n",
    "$$\\begin{array}{cc}\n",
    "\\frac{\\delta f_{i}}{x_{j}}=\\frac{\\delta f_{i}\\left(\\vec{X}^{(0)}\\right)}{x_{j}} & \\quad\\forall i,j=1,2,3\\end{array}$$\n",
    "\n",
    "También suponemos que la matriz Jacobiana es no singular, por lo que su inversa existe. Por lo tanto resolviendo para $H$ en (3), se tiene.\n",
    "\n",
    "$$\\vec{0}\\thickapprox F\\left(\\vec{X}^{(0)}\\right)+F'\\left(\\vec{X}^{(0)}\\right)\\vec{H}$$\n",
    "\n",
    "$$ \\Rightarrow F'\\left(\\vec{X}^{(0)}\\right)\\vec{H} \\thickapprox -F\\left(\\vec{X}^{(0)}\\right) $$\n",
    "\n",
    "$$ \\Rightarrow \\left[F'\\left(\\vec{X}^{(0)}\\right)\\right]^{-1}F'\\left(\\vec{X}^{(0)}\\right)\\vec{H} \\thickapprox-\\left[F'\\left(\\vec{X}^{(0)}\\right)\\right]^{-1}F\\left(\\vec{X}^{(0)}\\right) $$\n",
    "\n",
    "$$\\Rightarrow \\vec{H}\\thickapprox-\\left[F'\\left(\\vec{X}^{(0)}\\right)\\right]^{-1}F\\left(\\vec{X}^{(0)}\\right) \\tag{4}$$\n",
    "\n",
    "De tal manera que $\\vec{H}$ (delta en el contexto de las redes neuronales) nos dice cuanto se tiene que modificar la solución anterior para encontrar una nueva mejor solución y el término.\n",
    "\n",
    "$$\\left[F'\\left(\\vec{X}^{(0)}\\right)\\right]^{-1}$$\n",
    "\n",
    "Es la inversa de la matriz Jacobiana. En general el método de Newton se ve de la siguiente manera.\n",
    "\n",
    "$$ \\vec{X}^{(k+1)}=\\vec{X}^{(k)}-\\left[F'\\left(\\vec{X}^{(k)}\\right)\\right]^{-1}F\\left(\\vec{X}^{(k)}\\right)$$\n",
    "\n",
    "O de otra forma.\n",
    "\n",
    "$$ \\vec{X}^{(k+1)}=\\vec{X}^{(k)}-\\vec{H} \\tag{5}$$\n",
    "\n",
    "En la práctica el método de Newton **no implica invertir la matriz Jacobiana**, solo resolver los sistemas lineales jacobianos. Así que de (4) basta despejar $\\vec{H}$\n",
    "\n",
    "$$ \\left[F'\\left(\\vec{X}^{(k)}\\right)\\right]\\vec{H}^{(k)}=-F\\left(\\vec{X}^{(k)}\\right) \\tag{6}$$\n",
    "\n",
    "Podemos asegurar que $(6)$ representa un sistema lineal, ya que la incógnita es $\\vec{H}$ y los coeficientes de la matriz jacobiana no dependen $\\vec{H}$.\n",
    "\n",
    "Una vez encontrado el valor de $\\vec{H}$, solo hace falta restarlo a la aproximación anterior, para encontrar la aproximación siguiente, tal como muestra (5).\n",
    "\n",
    "Esto significa que para resolver un sistema de ecuaciones no lineales necesitamos resolver un sistema de ecuaciones lineales, es decir que **para resolver un problema que parece complejo, se resuelve un problema equivalente pero menos complejo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSjQ3bDLU-rh"
   },
   "source": [
    "## Ejemplo\n",
    "\n",
    "Emplea el método de Newton para resolver el siguiente sistema de ecuaciones con una tolerancia de $10^{-8}$.\n",
    "\n",
    "$$2x_{1}-x_{2}-e^{-x_{1}}=0, \\quad -x_{1}+2x_{2}-e^{-x_{2}}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T93clcxIVInd"
   },
   "source": [
    "### Solución\n",
    "\n",
    "Este sistema de ecuaciones se puede expresar en notación vectorial de la siguiente manera.\n",
    "\n",
    "\n",
    "$$F\\left(\\vec{X}\\right)=\\begin{cases}\n",
    "2x_{1}-x_{2}-e^{-x_{1}} & =0\\\\\n",
    "-x_{1}+2x_{2}-e^{-x_{2}} & =0\n",
    "\\end{cases}$$\n",
    "\n",
    "De tal manera que la matriz Jacobina asociada a este sistema esta definida por.\n",
    "\n",
    "$$\\begin{array}{cc}\n",
    "F'(\\vec{X})= & \\left(\\begin{array}{cc}\n",
    "2+e^{-x_{1}} & -1\\\\\n",
    "-1 & 2+e^{-x_{2}}\n",
    "\\end{array}\\right)\\end{array}$$\n",
    "\n",
    "Para calcular la inversa de $F'(\\vec{X})$ es necesario calcular el determinante, el cual esta dado por\n",
    "\n",
    "$$det(F'(\\vec{X}))=\\left(2+e^{-x_{1}}\\right)\\left(2+e^{-x_{2}}\\right)-\\left(-1\\times-1\\right)=4+2e^{-x_{2}}+2e^{-x_{1}}+e^{-x_{1}-x_{2}}-1$$\n",
    "\n",
    "Y la inversa de $F'(\\vec{X})$ se define como\n",
    "\n",
    "$$F'(\\vec{X})^{-1}=\\frac{1}{det(F'(\\vec{X}))}\\left(\\begin{array}{cc}\n",
    "2+e^{-x_{2}} & 1\\\\\n",
    "1 & 2+e^{-x_{1}}\n",
    "\\end{array}\\right)\\\\=\\left(\\begin{array}{cc}\n",
    "\\frac{2+e^{-x_{2}}}{4+2e^{-x_{2}}+2e^{-x_{1}}+e^{-x_{1}-x_{2}}-1} & \\frac{1}{4+2e^{-x_{2}}+2e^{-x_{1}}+e^{-x_{1}-x_{2}}-1}\\\\\n",
    "\\frac{1}{4+2e^{-x_{2}}+2e^{-x_{1}}+e^{-x_{1}-x_{2}}-1} & \\frac{2+e^{-x_{1}}}{4+2e^{-x_{2}}+2e^{-x_{1}}+e^{-x_{1}-x_{2}}-1}\n",
    "\\end{array}\\right) $$\n",
    "\n",
    "Ahora solo nos hace falta una primera aproximación\n",
    "\n",
    "$$ \\vec{X}^{(0)}=(0,0) $$\n",
    "\n",
    "No forzosamente tiene que ser el vector $\\vec{0}$, pero este método no tiene restricciones al respecto\n",
    "\n",
    "De manera tal que ahora podemos aplicar la definición del método de Newton para sistemas de ecuaciones y resolver para\n",
    "\n",
    "$$\\vec{X}^{(1)}=\\vec{X}^{(0)}-\\left[F'\\left(\\vec{X}^{(0)}\\right)\\right]^{-1}F\\left(\\vec{X}^{(0)}\\right)\\tag{5}$$\n",
    "\n",
    "Al evaluar $$\\vec{X}^{(0)}=(0,0)$$ en (5), se tiene\n",
    "\n",
    "$$\\vec{X}^{(1)}=\\left(0,0\\right)-\\left[F'\\left(0,0\\right)\\right]^{-1}F\\left(0,0\\right)$$\n",
    "\n",
    "$$=\\left(0,0\\right)-\\left(\\begin{array}{cc}\n",
    "\\frac{3}{8} & \\frac{1}{8}\\\\\n",
    "\\frac{1}{8} & \\frac{3}{8}\n",
    "\\end{array}\\right) \\left(\\begin{array}{c}\n",
    "-1\\\\\n",
    "-1\n",
    "\\end{array}\\right) $$\n",
    "\n",
    "$$ =\\left(0,0\\right)-\\left(\\begin{array}{c}\n",
    "\\left(\\frac{3}{8}\\cdot-1\\right)+\\left(\\frac{1}{8}\\cdot-1\\right)\\\\\n",
    "\\left(\\frac{1}{8}\\cdot-1\\right)+\\left(\\frac{3}{8}\\cdot-1\\right)\n",
    "\\end{array}\\right) $$\n",
    "\n",
    "$$=\\left(0,0\\right)-\\left(\\begin{array}{c}\n",
    "-\\frac{1}{2}\\\\\n",
    "-\\frac{1}{2}\n",
    "\\end{array}\\right) $$\n",
    "\n",
    "$$\\vec{X}^{(1)}=\\left(\\begin{array}{c}\n",
    "\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "Después de $k$ iteraciones con el método de Newton podemos ver que la solución converge al valor\n",
    "\n",
    "$$\\vec{X}^{(k)}=\\left(\\begin{array}{c}\n",
    "5.6714329040978384\\times10^{-1}\\\\\n",
    "5.6714329040978384\\times10^{-1}\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "Lo que significa que si evaluamos $F\\left(\\vec{X}^{(k)}\\right)$ obtendremos un vector muy cercano al vector $\\vec{0}$\n",
    "\n",
    "$$F\\left(\\vec{X}^{(k)}\\right)=\\left(\\begin{array}{c}\n",
    "0.000000001\\\\\n",
    "0.000000001\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "En *python* este algoritmo se ve asi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vdz_pIvQcxjD",
    "outputId": "ab3aca86-9c48-4906-dc13-0a3d33e622ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aproximación de la solución\n",
      "[0.56714329 0.56714329]\n",
      "Aproximación evaluada en f1\n",
      "-1.1102230246251565e-16\n",
      "Aproximación evaluada en f2\n",
      "-1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# función f1\n",
    "def f1(x1, x2):\n",
    "    return 2*x1-x2-math.e**(-x1)\n",
    "\n",
    "# función f2\n",
    "def f2(x1, x2):\n",
    "    return -x1+2*x2-math.e**(-x2)\n",
    "\n",
    "# denominador de la matriz jacobiana\n",
    "def denominador(x1,x2):\n",
    "    return 3+2*math.e**(-x2)+2*math.e**(-x1)+math.e**(-x1-x2)\n",
    "\n",
    "# parcial (en este caso la parcial de f1 y f2 es la misma)\n",
    "def par(x):\n",
    "    return 2+math.e**(-x)\n",
    "\n",
    "'''Método de Newton para sistemas\n",
    "   de ecuaciones no lineales\n",
    "   aprox: es la primera aproximación\n",
    "   en forma de vector'''\n",
    "def JimyNewtron(aprox):\n",
    "    # contador de iteraciones\n",
    "    n = 0\n",
    "    while n < 100: # maximo numero de iteraciones 100\n",
    "        '''es necesario calcular la matriz jacobiana\n",
    "        y para ello se necesita una matriz vacia de 2x2'''\n",
    "        jacobinv = np.zeros([2,2])\n",
    "\n",
    "        # valores de la matriz jacobiana en todas sus entradas\n",
    "        jacobinv[0][0] = par(aprox[1])/denominador(aprox[0], aprox[1])\n",
    "        jacobinv[0][1] = 1/denominador(aprox[0], aprox[1])\n",
    "        jacobinv[1][0] = 1/denominador(aprox[0], aprox[1])\n",
    "        jacobinv[1][1] = par(aprox[0])/denominador(aprox[0], aprox[1])\n",
    "\n",
    "        # guarda la evaluación de f1 y f2 en forma de vector\n",
    "        fx = np.array(aprox)\n",
    "        fx[0] = f1(aprox[0], aprox[1])\n",
    "        fx[1] = f2(aprox[0], aprox[1])\n",
    "\n",
    "        # FORMA ITERATIVA DEL MÉTODO DE NEWTON PARA SISTEMAS NO LINEALES\n",
    "        aprox = aprox - np.matmul(jacobinv, fx)\n",
    "\n",
    "        # se incrementa el contador\n",
    "        n+=1\n",
    "\n",
    "    # El valor devuelto es la aproximación\n",
    "    return aprox\n",
    "\n",
    "def main():\n",
    "    #Aproximación inicial (X=(0.0,0.0))\n",
    "    ap = np.zeros([2])\n",
    "    sol = JimyNewtron(ap)\n",
    "    print('Aproximación de la solución')\n",
    "    print(sol)\n",
    "    print('Aproximación evaluada en f1')\n",
    "    print(f1(sol[0], sol[1]))\n",
    "    print('Aproximación evaluada en f2')\n",
    "    print(f2(sol[0], sol[1]))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS2sGnusWylD"
   },
   "source": [
    "### Mejoras\n",
    "\n",
    "Puedes comprobar el resultado usando [wolfram alpha](https://www.wolframalpha.com/input/?i=solve++%282x-y-e%5E%7B-x%7D%3D0%2C+-x%2B2y-e%5E%7B-y%7D%3D0%29).\n",
    "\n",
    "Y vale la pena recalcar que el ejemplo y su implementación en código son **versiones del método de Newton que pueden ser mejoradas**.\n",
    "\n",
    "Una de estas mejoras corresponde a incluir **criterios adicionales de paro**, además del limite de iteraciones. Uno de estos criterios es medir la distancia entre la solución $\\vec{X}^{(k)}$ y la solución $\\vec{X}^{(k+1)}$ para determinar cuánto ha cambiado la solución.\n",
    "\n",
    "Otro criterio es medir la distancia entre $F(\\vec{X}^{(k+1)})$ y el vector $\\vec{0}$, si esa distancia es menor que una cierta **tolerancia**, podemos afirmar que este criterio se cumple.\n",
    "\n",
    "Para poder medir estas distancias es necesario comprender el concepto de **norma**, que puedes revisar en este <a href=\"../Normas/06_Normas.ipynb\">enlace</a>.\n",
    "\n",
    "Por otro lado es importante recordar que el cálculo de la matriz inversa es muy costoso por lo tanto, otra mejora que se puede realizar es, **substituir el cálculo de la matriz inversa**, y en su lugar resolver el **sistema lineal Jacobiano** descrito en la sección de [Desarrollo](#scrollTo=ORb_b64xcxjA).\n",
    "\n",
    "\n",
    "Finalmente, hay que notar que el cálculo de las derivadas parciales se realiza en código duro ([*hard code*](https://es.wikipedia.org/wiki/Hard_code#:~:text=Hard%2Dcode%2C%20t%C3%A9rmino%20del%20mundo,par%C3%A1metros%20de%20la%20l%C3%ADnea%20de)), pero la definición de `def par(x)`, puede sustituirse por una función que aproxime la derivada parcial de cualquier función $F(\\vec{X})$, digamos `def parcial(F, X, var, Tol)` y que **devuelva la aproximación de la derivada parcial** de `F`, en el punto `X` con respecto a la variable `var` y con una tolerancia de `Tol`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hs7WdzQTnG-P"
   },
   "source": [
    "# Referencias\n",
    "\n",
    "1.Riswan Butt:Numerical Analysis Using Matlab, Jones and Bartlett.\n",
    "\n",
    "2.Ward Cheney, David Kincaid: Métodos Numéricos y Computación, Cenage Learning.\n",
    "\n",
    "3.Richard L. Burden, J. Douglas Faires: Análisis Numérico, Math Learning."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "01_SolucionEcuacionesNoLineales.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
